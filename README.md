## Overview  
This repository implements an end‑to‑end medallion data pipeline orchestrated by **Apache Airflow** and powered by **Apache Spark**. It extracts SISVAN microdata from BigQuery and loads it into **bronze**, **silver**, and **gold** layers in **Delta Lake** on **MinIO**, with analytical access via **Trino** and dashboards in **Apache Superset**.

## Architecture  
- The project follows the *Medallion Architecture*, organizing data flow into three incremental layers (bronze, silver, gold) to progressively improve data quality.  
- Orchestration is handled by Apache Airflow, allowing you to define Python DAGs for scheduling and monitoring tasks.  
- Large‑scale processing uses Apache Spark, which provides DataFrame APIs and integration with libraries like Delta Lake.  
- Delta Lake tables ensure ACID transactions and data versioning at each stage.  
- Object storage compatible with S3 is provided by MinIO, deployable locally or in the cloud.  
- For analytical queries, Trino offers a high‑performance distributed SQL engine for data lakes.  
- Visualization is done with Apache Superset, a modern, extensible BI platform for creating interactive dashboards.

## Technologies  
- **Apache Airflow** (orchestration)  
- **Astro CLI** (local Airflow development)  
- **Apache Spark** (distributed processing)  
- **Delta Lake** (transactional storage)  
- **MinIO** (S3‑compatible object storage)  
- **Trino** (SQL engine for data lakes)  
- **Apache Superset** (BI and dashboards)

## Requirements  
- Docker >= 20.10 and Docker Compose  
- Astro CLI for local Airflow development  
- Python 3.8+ (for Airflow dependencies)  
- Google Cloud project with BigQuery access and a service account key (JSON)  
- MinIO endpoint URL, access key, and secret key

## Repository Structure  
```text
.
├── .astro/                          # Configurations generated by Astro CLI  
├── dags/                            # Airflow DAGs (e.g. bigquery_to_bronze.py)  
├── include/                         # Helper scripts (e.g. Spark job scripts)  
├── Dockerfile                       # Custom Astro runtime image  
├── docker-compose.override.yml      # Overrides for MinIO, Spark, etc.  
├── packages.txt                     # OS‑level dependencies for Docker  
├── requirements.txt                 # Python dependencies for Airflow  
└── README.md                        # This file  
```

## Installation & Run  
1. **Clone the repo**  
   ```bash
   git clone https://github.com/italovinicius18/sisvan-lakehouse-pipeline.git
   cd sisvan-lakehouse-pipeline
   ```  
2. **Install Astro CLI** (if not already installed)  
3. **Start the environment**  
   ```bash
   astro dev start --build
   ```  
   This will launch containers for Postgres, Airflow webserver, scheduler, triggerer, as well as MinIO and Spark defined in `docker-compose.override.yml`.  

4. **Access Airflow UI**  
   Visit `http://localhost:8080/` and log in with **admin/admin**.

## Configuration  
Set the following in `airflow_settings.yaml` or as environment variables before running:  
- `AIRFLOW_CONN_SPARK_DEFAULT` — Spark connection URI  
- `GCP_SERVICE_ACCOUNT_KEY` — Path to your BigQuery service account JSON  
- `S3_ENDPOINT_URL`, `S3_ACCESS_KEY`, `S3_SECRET_KEY` — MinIO connection details

## Main DAGs  
- **bigquery_to_bronze**: Extracts SISVAN microdata from BigQuery and writes to `s3a://bronze/sisvan/` as Delta Lake.  
- **bronze_to_silver**: (in progress) Cleans and transforms raw data.  
- **silver_to_gold**: (in progress) Aggregates and enriches data for BI.

## Contributing  
1. Fork this repository  
2. Create a feature branch:  
   ```bash
   git checkout -b feature/your-feature
   ```  
3. Commit your changes:  
   ```bash
   git commit -m "Add your feature"
   ```  
4. Push to your branch:  
   ```bash
   git push origin feature/your-feature
   ```  
5. Open a Pull Request on GitHub.

## License  
This project is licensed under the **MIT License**. See the `LICENSE` file for details.

---

Ready to explore SISVAN insights in a true **Medallion Lakehouse**?
